const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const compression = require('compression');
const { createServer } = require('http');
const { Server } = require('socket.io');
const path = require('path');
const fs = require('fs');

// Initialize Express app
const app = express();
const server = createServer(app);
const io = new Server(server, {
  cors: {
    origin: "*",
    methods: ["GET", "POST", "PUT", "DELETE"]
  }
});

// Configuration
const PORT = process.env.PORT || 3000;
const NODE_ENV = process.env.NODE_ENV || 'development';

// Middleware
app.use(helmet({
  contentSecurityPolicy: false,
  crossOriginEmbedderPolicy: false
}));
app.use(cors());
app.use(compression());
app.use(express.json({ limit: '50mb' }));
app.use(express.urlencoded({ extended: true, limit: '50mb' }));

// Serve static files
app.use(express.static(path.join(__dirname, 'public')));
app.use('/assets', express.static(path.join(__dirname, 'assets')));

// AI Services Configuration
const aiServices = {
  puter: {
    enabled: true,
    url: 'https://js.puter.com/v2/',
    models: ['claude-3-5-sonnet', 'gpt-4o', 'o3-mini', 'gemini-pro'],
    status: 'initializing'
  },
  huggingface: {
    enabled: true,
    url: 'https://api-inference.huggingface.co',
    models: ['microsoft/DialoGPT-medium', 'gpt2', 'distilbert-base-uncased'],
    status: 'initializing'
  },
  openrouter: {
    enabled: true,
    url: 'https://openrouter.ai/api/v1',
    models: ['meta-llama/llama-3.1-8b-instruct:free', 'microsoft/phi-3-mini-128k-instruct:free'],
    status: 'initializing'
  },
  local: {
    enabled: true,
    models: ['local-gpt', 'fallback-ai'],
    status: 'ready'
  }
};

// System status
let systemStatus = {
  server: 'starting',
  ai: 'initializing',
  network: 'connecting',
  storage: 'ready',
  uptime: Date.now()
};

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    uptime: Date.now() - systemStatus.uptime,
    services: aiServices,
    system: systemStatus,
    timestamp: new Date().toISOString()
  });
});

// API Routes
app.get('/api/status', (req, res) => {
  res.json({
    success: true,
    system: systemStatus,
    ai: aiServices,
    timestamp: new Date().toISOString()
  });
});

// AI Chat endpoint with multiple fallbacks
app.post('/api/chat', async (req, res) => {
  try {
    const { message, model = 'auto', temperature = 0.7 } = req.body;
    
    if (!message) {
      return res.status(400).json({ error: 'Message is required' });
    }

    let response = null;
    let usedService = null;

    // Try Puter.js first (free unlimited)
    if (aiServices.puter.status === 'ready') {
      try {
        response = await callPuterAI(message, model, temperature);
        usedService = 'puter';
      } catch (error) {
        console.warn('Puter AI failed:', error.message);
      }
    }

    // Fallback to HuggingFace
    if (!response && aiServices.huggingface.status === 'ready') {
      try {
        response = await callHuggingFaceAI(message, model);
        usedService = 'huggingface';
      } catch (error) {
        console.warn('HuggingFace AI failed:', error.message);
      }
    }

    // Fallback to OpenRouter
    if (!response && aiServices.openrouter.status === 'ready') {
      try {
        response = await callOpenRouterAI(message, model);
        usedService = 'openrouter';
      } catch (error) {
        console.warn('OpenRouter AI failed:', error.message);
      }
    }

    // Final fallback to local AI
    if (!response) {
      response = generateLocalResponse(message);
      usedService = 'local';
    }

    res.json({
      success: true,
      response,
      service: usedService,
      model: model,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('Chat API error:', error);
    res.status(500).json({
      error: 'AI service temporarily unavailable',
      fallback: generateLocalResponse(req.body.message || 'Hello'),
      timestamp: new Date().toISOString()
    });
  }
});

// Code generation endpoint
app.post('/api/generate-code', async (req, res) => {
  try {
    const { description, language = 'javascript', framework = 'vanilla' } = req.body;
    
    const prompt = `Generate ${language} code for: ${description}
    
Requirements:
- Use ${framework} framework
- Include error handling
- Add comments and documentation
- Follow best practices
- Make it production-ready

Return only the code, no explanations.`;

    const response = await callBestAvailableAI(prompt, 'claude-3-5-sonnet');
    
    res.json({
      success: true,
      code: response,
      language,
      framework,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('Code generation error:', error);
    res.status(500).json({
      error: 'Code generation failed',
      fallback: generateCodeFallback(req.body.description, req.body.language)
    });
  }
});

// File analysis endpoint
app.post('/api/analyze-file', async (req, res) => {
  try {
    const { content, filename, type } = req.body;
    
    const prompt = `Analyze this ${type || 'file'} (${filename}):

${content}

Provide:
1. Code quality assessment
2. Security analysis
3. Performance recommendations
4. Best practices suggestions
5. Potential improvements

Be concise but thorough.`;

    const analysis = await callBestAvailableAI(prompt, 'claude-3-5-sonnet');
    
    res.json({
      success: true,
      analysis,
      filename,
      type,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('File analysis error:', error);
    res.status(500).json({
      error: 'File analysis failed',
      fallback: `Basic analysis for ${req.body.filename}: File appears to be valid ${req.body.type} code.`
    });
  }
});

// Network discovery endpoint
app.get('/api/network/discover', (req, res) => {
  res.json({
    success: true,
    nodes: [
      {
        id: 'local',
        name: 'Local Node',
        status: 'active',
        services: ['ai', 'storage', 'compute'],
        endpoint: `http://localhost:${PORT}`
      }
    ],
    timestamp: new Date().toISOString()
  });
});

// AI service helper functions
async function callPuterAI(message, model, temperature) {
  // Simulated Puter.js call - in real implementation, this would use actual Puter.js SDK
  return new Promise((resolve) => {
    setTimeout(() => {
      resolve(`AI Response: ${message} (via Puter.js ${model})`);
    }, 500);
  });
}

async function callHuggingFaceAI(message, model) {
  // Simulated HuggingFace call
  return new Promise((resolve) => {
    setTimeout(() => {
      resolve(`AI Response: ${message} (via HuggingFace ${model})`);
    }, 800);
  });
}

async function callOpenRouterAI(message, model) {
  // Simulated OpenRouter call
  return new Promise((resolve) => {
    setTimeout(() => {
      resolve(`AI Response: ${message} (via OpenRouter ${model})`);
    }, 600);
  });
}

async function callBestAvailableAI(prompt, preferredModel) {
  // Try services in order of preference
  for (const [service, config] of Object.entries(aiServices)) {
    if (config.status === 'ready' && config.enabled) {
      try {
        switch (service) {
          case 'puter':
            return await callPuterAI(prompt, preferredModel, 0.7);
          case 'huggingface':
            return await callHuggingFaceAI(prompt, preferredModel);
          case 'openrouter':
            return await callOpenRouterAI(prompt, preferredModel);
          default:
            continue;
        }
      } catch (error) {
        console.warn(`${service} failed:`, error.message);
        continue;
      }
    }
  }
  
  // Final fallback
  return generateLocalResponse(prompt);
}

function generateLocalResponse(message) {
  const responses = [
    `I understand you're asking about: "${message}". While AI services are connecting, I can provide basic assistance.`,
    `Regarding "${message}", I'm processing your request locally. AI services will enhance responses once connected.`,
    `Your query about "${message}" is noted. Local processing active, enhanced AI features loading.`,
    `Processing "${message}" locally. Full AI capabilities will be available shortly.`
  ];
  
  return responses[Math.floor(Math.random() * responses.length)];
}

function generateCodeFallback(description, language) {
  return `// ${language} code for: ${description}
// Generated by GRUDA Legion local system
// TODO: Implement ${description}

function main() {
    console.log('${description} implementation needed');
    // Add your implementation here
}

main();`;
}

// WebSocket handling
io.on('connection', (socket) => {
  console.log(`Client connected: ${socket.id}`);
  
  // Send initial status
  socket.emit('system-status', systemStatus);
  socket.emit('ai-services', aiServices);
  
  socket.on('chat-message', async (data) => {
    try {
      const response = await callBestAvailableAI(data.message, data.model);
      socket.emit('chat-response', {
        id: data.id,
        response,
        timestamp: new Date().toISOString()
      });
    } catch (error) {
      socket.emit('chat-error', {
        id: data.id,
        error: error.message
      });
    }
  });
  
  socket.on('disconnect', () => {
    console.log(`Client disconnected: ${socket.id}`);
  });
});

// Initialize AI services
async function initializeAIServices() {
  console.log('ü§ñ Initializing AI services...');
  
  // Initialize Puter.js
  try {
    aiServices.puter.status = 'ready';
    console.log('‚úÖ Puter.js AI service ready');
  } catch (error) {
    console.warn('‚ö†Ô∏è Puter.js initialization failed:', error.message);
    aiServices.puter.status = 'error';
  }
  
  // Initialize HuggingFace
  try {
    aiServices.huggingface.status = 'ready';
    console.log('‚úÖ HuggingFace AI service ready');
  } catch (error) {
    console.warn('‚ö†Ô∏è HuggingFace initialization failed:', error.message);
    aiServices.huggingface.status = 'error';
  }
  
  // Initialize OpenRouter
  try {
    aiServices.openrouter.status = 'ready';
    console.log('‚úÖ OpenRouter AI service ready');
  } catch (error) {
    console.warn('‚ö†Ô∏è OpenRouter initialization failed:', error.message);
    aiServices.openrouter.status = 'error';
  }
  
  systemStatus.ai = 'ready';
  console.log('üéâ All AI services initialized');
}

// Start server
server.listen(PORT, async () => {
  console.log(`
  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó
 ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë
 ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë
 ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë
 ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë
  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù
  `);
  
  console.log('üöÄ GRUDA Legion Server Started');
  console.log(`üì° Server running on http://localhost:${PORT}`);
  console.log(`üåê Environment: ${NODE_ENV}`);
  console.log(`‚ö° Free AI services enabled`);
  
  systemStatus.server = 'running';
  systemStatus.network = 'connected';
  
  // Initialize AI services
  await initializeAIServices();
  
  console.log(`
üéØ Access Points:
‚Ä¢ Main Interface: http://localhost:${PORT}
‚Ä¢ Health Check:   http://localhost:${PORT}/health
‚Ä¢ API Status:     http://localhost:${PORT}/api/status
‚Ä¢ WebSocket:      ws://localhost:${PORT}

ü§ñ AI Services Available:
‚Ä¢ Puter.js (Claude, GPT-4o, O3-mini) - Free unlimited
‚Ä¢ HuggingFace Inference API - Free tier
‚Ä¢ OpenRouter Free Models - Free tier
‚Ä¢ Local AI Fallback - Always available

‚úÖ GRUDA Legion is fully operational!
  `);
});

// Graceful shutdown
process.on('SIGINT', () => {
  console.log('\nüõë Shutting down GRUDA Legion server...');
  server.close(() => {
    console.log('‚úÖ Server closed gracefully');
    process.exit(0);
  });
});

process.on('uncaughtException', (error) => {
  console.error('‚ùå Uncaught Exception:', error);
  process.exit(1);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('‚ùå Unhandled Rejection at:', promise, 'reason:', reason);
  process.exit(1);
});

module.exports = app;
